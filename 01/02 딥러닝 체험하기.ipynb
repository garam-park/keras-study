{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 import\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1단계 데이터셋 생성하기\n",
    "\n",
    "- 학습 시키려는 데이터를 로딩\n",
    "- 학습 시키려는 데이터를 생성\n",
    "\n",
    "여기서는 mnist 라는 유명한 손글씨 데이터를 가지고 오도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트레이닝 데이터셋 인풋 : \n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "트레이닝 데이터셋 정답 : 5\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로딩\n",
    "(x_train, y_train) , (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "np.set_printoptions(linewidth=np.inf)# 한줄 출력을 위한 것\n",
    "\n",
    "# 그중에 첫번째 데이터가 무엇인가를 확인\n",
    "print(\"트레이닝 데이터셋 인풋 : \")\n",
    "print(x_train[0])\n",
    "print(\"트레이닝 데이터셋 정답 : \"+ str(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist 는 6만개의 학습셋과 1만개의 테스트 셋으로 구성 되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.         0.96862745 0.49803922 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.11764706 0.14117648 0.36862746 0.6039216  0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.19215687 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864 0.32156864 0.21960784 0.15294118 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.07058824 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255  0.96862745 0.94509804 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.3137255  0.6117647  0.41960785 0.99215686 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.54509807 0.99215686 0.74509805 0.00784314 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.04313726 0.74509805 0.99215686 0.27450982 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.13725491 0.94509804 0.88235295 0.627451   0.42352942 0.00392157 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.31764707 0.9411765  0.99215686 0.99215686 0.46666667 0.09803922 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.1764706  0.7294118  0.99215686 0.99215686 0.5882353  0.10588235 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.9764706  0.99215686 0.9764706  0.2509804  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.18039216 0.50980395 0.7176471  0.99215686 0.99215686 0.8117647  0.00784314 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.15294118 0.5803922  0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686 0.99215686 0.7882353  0.30588236 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.09019608 0.25882354 0.8352941  0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.07058824 0.67058825 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059  0.3137255  0.03529412 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.53333336 0.99215686 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 한줄로 쭉 펴준다\n",
    "\n",
    "x_train = x_train.reshape(60000,784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000,784).astype('float32') / 255.0\n",
    "\n",
    "# x_train = x_train.reshape(-1,784).astype('float32')\n",
    "# x_test = x_test.reshape(-1,784).astype('float32')\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원 핫 인코딩으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 단계 모델 구성하기\n",
    "\n",
    "입력층과 출력층이 있는 간단한 모델을 구성해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64,input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3단계 모델 학습과정 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4단계 모델 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.6804 - acc: 0.8284\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.3407 - acc: 0.9041\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2939 - acc: 0.9160\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2655 - acc: 0.9243\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2438 - acc: 0.9306\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2259 - acc: 0.9357\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2105 - acc: 0.9407\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1972 - acc: 0.9448\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1860 - acc: 0.9470\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1757 - acc: 0.9501\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1665 - acc: 0.9533\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1586 - acc: 0.9547\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1517 - acc: 0.9576\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1450 - acc: 0.9591\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.1388 - acc: 0.960 - 1s 20us/step - loss: 0.1392 - acc: 0.9606\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1337 - acc: 0.9623\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1286 - acc: 0.9638\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1240 - acc: 0.9652\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1199 - acc: 0.9664\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1158 - acc: 0.9677\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1122 - acc: 0.9682\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1087 - acc: 0.9692\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1054 - acc: 0.9701\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1024 - acc: 0.9711\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0997 - acc: 0.9716\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0969 - acc: 0.9725\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0946 - acc: 0.9730\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0921 - acc: 0.9741\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0897 - acc: 0.9746\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0876 - acc: 0.9756\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0856 - acc: 0.9762\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0836 - acc: 0.9770\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0818 - acc: 0.9774\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0801 - acc: 0.9782\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0784 - acc: 0.9782\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0768 - acc: 0.9790: 0s - loss: 0.0767 - acc: 0.979\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0753 - acc: 0.9791\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0737 - acc: 0.9796\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0723 - acc: 0.9802\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0709 - acc: 0.9807\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0696 - acc: 0.9808\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0684 - acc: 0.9815\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0672 - acc: 0.9819\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0659 - acc: 0.9818\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0647 - acc: 0.9826\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0637 - acc: 0.9829\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0626 - acc: 0.9831\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0616 - acc: 0.9832\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0607 - acc: 0.9834\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0596 - acc: 0.9836\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0587 - acc: 0.9843\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0579 - acc: 0.9848\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0569 - acc: 0.9849\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0560 - acc: 0.9849\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0550 - acc: 0.9850\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0544 - acc: 0.9855\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0535 - acc: 0.9856\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0526 - acc: 0.9859\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0519 - acc: 0.9862\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0512 - acc: 0.9867\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0504 - acc: 0.9868\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0496 - acc: 0.9871\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0491 - acc: 0.9872\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0482 - acc: 0.9876\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0475 - acc: 0.9878\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0469 - acc: 0.9875\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0462 - acc: 0.9881\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0457 - acc: 0.9884\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0451 - acc: 0.9886\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0445 - acc: 0.9885\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0439 - acc: 0.9888\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0433 - acc: 0.9890\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0427 - acc: 0.9890\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0422 - acc: 0.9894\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0417 - acc: 0.9892\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0410 - acc: 0.9898\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0406 - acc: 0.9901: 0s - loss: 0.0397 - acc:\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0401 - acc: 0.9899\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0396 - acc: 0.9902\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0390 - acc: 0.9905\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0386 - acc: 0.9907\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0380 - acc: 0.9905\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0377 - acc: 0.9908\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0372 - acc: 0.9910\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0367 - acc: 0.9913\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0362 - acc: 0.9913\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0357 - acc: 0.9915\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0354 - acc: 0.9916\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0349 - acc: 0.9918\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0345 - acc: 0.9921\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0340 - acc: 0.9923\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0336 - acc: 0.9924\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0333 - acc: 0.9925\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0329 - acc: 0.9925\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0325 - acc: 0.9925: 0s - loss: 0.03\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0320 - acc: 0.9929\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0317 - acc: 0.9929\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0313 - acc: 0.9932\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0310 - acc: 0.9934\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0305 - acc: 0.9933\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train,y_train,epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5  단계 학습과정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6804139303485552, 0.3407434992512067, 0.293933588685592, 0.26553983776172, 0.24378695451418558, 0.2259015962064266, 0.2104771410961946, 0.19722544400791328, 0.1859502805372079, 0.17568728145261606, 0.16652095077633858, 0.15864784305294355, 0.15167587526887655, 0.14500639741569757, 0.13922734645505747, 0.1337443397089839, 0.12860041798204183, 0.12403766309171915, 0.11986998798499504, 0.1157762305577596, 0.11218396802395582, 0.1087376074001193, 0.10543086368938287, 0.10242060128897429, 0.09973583492860198, 0.0969138585664332, 0.09462280704999963, 0.09210547064393759, 0.08971821000600855, 0.08764100398967664, 0.08555198883687457, 0.08357964648505052, 0.081822563012441, 0.08007577159230908, 0.07839560136223833, 0.0767732985506455, 0.0753233414883415, 0.07371758064205448, 0.07226801368867358, 0.07091684971352419, 0.06959002556068201, 0.06838236949245136, 0.06719933866535624, 0.06591973305580516, 0.06467352732457221, 0.06373101586264869, 0.06260208520392577, 0.06163482335228473, 0.06074303625697891, 0.05962851457285384, 0.058668897345165415, 0.05788047244225939, 0.05689941290076822, 0.056010531740138925, 0.05501474491556486, 0.054353437981630365, 0.053462417592977485, 0.052580857378120224, 0.05187735874305169, 0.05115206501757105, 0.05041010350926469, 0.04961028219014406, 0.04905612638778985, 0.04823206050954759, 0.04752723842567454, 0.04693804753081252, 0.04623614964379619, 0.04572643507594864, 0.04507088960371911, 0.04452402940187603, 0.04388361541057626, 0.04328754540955027, 0.042727039476297794, 0.04221538585955277, 0.041664470751583575, 0.041046939807664606, 0.04056378529512634, 0.04008425290895005, 0.03958218210885922, 0.03898215581430122, 0.038589719966674846, 0.03802121913811813, 0.037665771489031614, 0.03715837838137522, 0.03665745138979207, 0.03618698922758922, 0.035682073352610075, 0.03538763152640313, 0.034944701884003976, 0.03449636373780668, 0.03401621810210248, 0.03355443928390742, 0.033270092306565496, 0.032879612234545254, 0.03246931106063227, 0.03197500986283024, 0.03167994574109713, 0.03134013896814237, 0.030953693259134888, 0.03054467179523781]\n",
      "[0.8283666666666667, 0.9041, 0.9159666666666667, 0.9242666666666667, 0.9306166666666666, 0.9357166666666666, 0.94075, 0.9448333333333333, 0.9470166666666666, 0.9500666666666666, 0.9532666666666667, 0.9547166666666667, 0.95755, 0.95915, 0.9606333333333333, 0.9623166666666667, 0.9637833333333333, 0.9651833333333333, 0.9663666666666667, 0.9676666666666667, 0.96825, 0.9692166666666666, 0.9700666666666666, 0.9710833333333333, 0.9715666666666667, 0.9724666666666667, 0.97295, 0.9740666666666666, 0.9745833333333334, 0.9755833333333334, 0.97625, 0.977, 0.9774166666666667, 0.9782, 0.9781833333333333, 0.97905, 0.97915, 0.9796, 0.9802, 0.9806833333333334, 0.9807666666666667, 0.9815333333333334, 0.9818833333333333, 0.98185, 0.9825833333333334, 0.98285, 0.9830833333333333, 0.9832, 0.9834, 0.9836333333333334, 0.9843, 0.9847833333333333, 0.9848833333333333, 0.9849, 0.985, 0.9855, 0.9855833333333334, 0.9858833333333333, 0.9862166666666666, 0.9867333333333334, 0.98675, 0.98715, 0.9872166666666666, 0.9876, 0.9878, 0.9875333333333334, 0.9881333333333333, 0.9884333333333334, 0.9885833333333334, 0.9885166666666667, 0.98875, 0.9889666666666667, 0.9890166666666667, 0.9893666666666666, 0.9891833333333333, 0.9897666666666667, 0.9901, 0.9899166666666667, 0.9901666666666666, 0.9904833333333334, 0.9906666666666667, 0.9905333333333334, 0.9908333333333333, 0.9910166666666667, 0.9912666666666666, 0.99135, 0.99155, 0.99165, 0.9917666666666667, 0.9921166666666666, 0.9923, 0.9924, 0.9925, 0.9924833333333334, 0.9925333333333334, 0.99295, 0.99295, 0.99325, 0.9934166666666666, 0.9933]\n"
     ]
    }
   ],
   "source": [
    "print(hist.history['loss'])\n",
    "print(hist.history['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6단계 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 11us/step\n",
      "[0.08191094595703761, 0.9749]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(x_test,y_test,batch_size=32)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7단계 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 0 0]]\n",
      "[[0 0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "xhat = x_test[0:1]\n",
    "y = y_test[0:1]\n",
    "\n",
    "yhat = model.predict(xhat)\n",
    "\n",
    "print((yhat*10/9).astype('int'))\n",
    "print(y.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
